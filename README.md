# text-similarity
for CIKM2018

第一次尝试用深度学习参加NLP比赛，记录下心得

【关于跨语言】
该模型主要借助了facebook的MUSE，基于训练好的词向量模型和词对齐训练集（可选），将不同语言的词向量映射在同一个向量空间中，以此实现跨语言的迁移。
在模型使用中，尝试了直接把西班牙语和英语一起训练，在单独预测西语，效果不好。最终使用迁移学习的方法，首先训练英语，再在训练好的模型的基础上训练西班牙语。

【关于模型】
该比赛数据集特别小，与隔壁kaggle的quora question pairs差距简直大，以至于我屡次想放弃使用集成学习方法搞。。。
在损失函数方面，尝试了将模型激活后的输出进行L2标准化，使用余弦相似度构建距离，再使用对比损失产生误差构造损失。又尝试了将模型输出相加与相乘拼接输入dense的方法用交叉熵构造损失。实践证明后者效果更好，大概是因为模型输出不够鲁棒吧，或者余弦相似度没能考虑维度间的关系，考虑更换距离函数。
首先尝试了CNN和biGRU的排列组合，发现分类效果不如单CNN，后来尝试了RNN的自注意、乘性注意和类似Attention Over Attention的方法，实践证明后者好用
模型最终使用bi-GRU + Attention的方法

【关于正负样本不均衡】
正负比例貌似是1比5
尝试了构造weight加权损失的方法，效果不好
尝试下采样，效果不好
很好奇大佬都是咋处理的

【关于其他特征】
kaggle比赛中有人将其他分类器产生的结果拼接在模型输出中，共同放入dense进行分类，比赛后期稍微尝试了一下
采用的分类器有
基于tfidf的lsi 、 是否包含否定词 、 fuzzywuzzy 、是否是问句 、 pagerank 、 长度差 等
实践了一下其实并没有多少提升   分类器都在feature.py里

由于实验室内存太小，直接加载词向量之后内存就差不多满了，无奈只能转换为词向量保存为文件，模型训练和调用时直接使生成器，在生成的词向量文件中按行读取即可，LoadData用于保存预加载的词向量文件。
